{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "from scipy.sparse import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image reconstruction in X-ray tomography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 X-ray tomography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q1.}}$ We start by loading the matrix $H$ and the image matrix $\\bar{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = loadmat(\"data/x.mat\")['x']\n",
    "H = loadmat(\"data/H.mat\")['H']\n",
    "G = loadmat(\"data/H.mat\")['H']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q2.}}$ Let's Construct y using such that:\n",
    "$$y = H\\bar{x} + \\omega $$\n",
    "where $\\omega \\in \\mathbb{R}^{M}$ is an i.i.d Gaussian noise with variance $\\sigma^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = H.shape[0]\n",
    "N = x.shape[0]\n",
    "\n",
    "sigma = 1\n",
    "lamb = 0.13\n",
    "delta = 0.02\n",
    "\n",
    "w = np.random.normal(scale = sigma, size=(M,1))\n",
    "y = H*x+w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q3.}}$ Let's visualize the image $\\bar{x}$, and a 2D version of $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.reshape(90,90).T,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y.reshape(180,90).T,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y,G,H,lamb,delta):\n",
    "    return 0.5*np.linalg.norm(H*x - y)**2 + lamb*r(x,delta,G) \n",
    "\n",
    "def grad_f(x,y,G,H,lamb,delta):\n",
    "    return H.T*(H*x - y) + lamb * grad_r(x,delta,G)\n",
    "\n",
    "def r(x,delta,G):\n",
    "    return float(np.sum(psi(G*x,delta),axis=0))\n",
    "\n",
    "def grad_r(x,delta,G):\n",
    "    return G.T*psi_prime(G*x,delta)\n",
    "\n",
    "def psi(u,delta):\n",
    "    return np.sqrt(1+np.power(u,2)/delta**2)\n",
    "\n",
    "def psi_prime(u,delta):\n",
    "    return u/(delta**2 * psi(u,delta))\n",
    "\n",
    "\n",
    "def SNR(x_hat,x):\n",
    "    return 10 * np.log10(np.linalg.norm(x)**2/np.linalg.norm(x-x_hat)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q2.}}$ Let be $x \\in \\mathbb{R}^{N}$, and let's compute $\\nabla f(x)$\n",
    "\n",
    "$$\\nabla f(x) = H^{T}(Hx - y) + \\lambda \\nabla r(x) = H^{T}(Hx - y)  + G^{T}\\Psi'(Gx)$$\n",
    "Where $\\Psi'(Gx) = \\left(\\Psi'([Gx]^{n})\\right)_{1\\leq n \\leq 2N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q3.}}$ Let's show that $\\nabla f $ is $L$-Lipshitz with $L = \\left\\| H\\right\\|^{2} + \\frac{\\lambda}{\\delta^{2}}\\left\\|G\\right\\|^{2}$. \n",
    "\n",
    "We know that $\\forall x_1,x_2 \\in \\mathbb{R}^{N}$ $$\\left\\|\\nabla f(x_1) - \\nabla f(x_2)\\right\\| \\leq \\underset{\\mathbb{R}^{N}}{\\text{sup}}\\left\\|\\nabla^{2}f\\right\\| . \\left\\|x_1-x_2\\right\\|$$ \n",
    "\n",
    "A simple computation show that, $\\forall x \\in \\mathbb{R}^{N}$\n",
    "\n",
    "$$\\nabla^{2}f(x) = H^{T}H + \\lambda G^{T}.\\text{Diag}\\left(\\Psi''(Gx)\\right).G$$\n",
    "\n",
    "And since $\\forall u \\in \\mathbb{R}$,  $\\Psi''(u) = \\frac{1}{\\delta^{2}\\sqrt{1+\\frac{u^{2}}{\\delta^{2}}}}\\left( 1 - \\frac{1}{\\delta^2 + u^2}\\right)$ . $\\forall x \\in \\mathbb{R}^{N}$, $\\left\\|\\nabla^{2} f(x)\\right\\| \\leq \\left\\|H\\right\\|^{2} + \\frac{\\lambda}{\\delta^2}\\left\\|G\\right\\|^{2}$\n",
    "\n",
    "Therfore $\\nabla f$ is $L$-lipshitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = scipy.sparse.linalg.norm(H)**2 + delta/lamb**2 * scipy.sparse.linalg.norm(G)**2\n",
    "print('L = {}'.format(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gradient descent algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q1.}}$ We create first $x_{0} \\in \\mathbb{R}^{N}$ a vector with all entries equal to 0. This will be our initialization for all tested algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros((N,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q2.}}$ We implement now a gradient descent algorithm to minimize $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,grad_f,x0,L,N,y,G,H,lamb,delta):\n",
    "    gamma = 1/L\n",
    "    grad = N*1e-4\n",
    "    x = x0\n",
    "    it = 0\n",
    "    itermax = 1000 #5000\n",
    "    objectives = []\n",
    "    times = [0]\n",
    "    \n",
    "    while grad > np.sqrt(N)*1e-4 and it <= itermax: \n",
    "        \n",
    "        t1 = time.time()\n",
    "        gradient = grad_f(x,y,G,H,lamb,delta)\n",
    "        x = x - gamma*gradient\n",
    "        grad = np.linalg.norm(gradient)\n",
    "        criterion = f(x,y,G,H,lamb,delta)        \n",
    "        objectives.append(criterion)\n",
    "        \n",
    "        if it%100 == 0:\n",
    "            print(\"Iteration {}, objective: {:.2E}, {}\".format(it,Decimal(criterion),grad))          \n",
    "        t2 = time.time()\n",
    "        times.append(times[-1]+t2-t1)\n",
    "        it += 1\n",
    "    return x,objectives,times\n",
    "        \n",
    "x_hat,objectives,times = gradient_descent(f,grad_f,x0,L,N,y,G,H,0.01,delta)\n",
    "plt.figure()\n",
    "plt.semilogy(times[1:],objectives)\n",
    "plt.grid(which='both')\n",
    "print('\\nSignal to Noise Ratio:',SNR(x_hat,x))\n",
    "plt.figure()\n",
    "plt.imshow(x_hat.reshape(90,90).T,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. MM quadratic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q1.}}$We define $$A(x) = H^{T}H + \\frac{1}{\\lambda}G^{T}\\omega(x)G$$\n",
    "Where $\\omega(x) = \\left( \\frac{\\psi'\\left([Gx]^{(n)}\\right)}{[Gx]^{n}}\\right)_{0\\leq n\\leq 2N}$\n",
    "\n",
    "Then a quadratic majorant function of $f$ is given by $$h(x,y) = f(y) + \\left\\langle \\nabla f(y) | x-y \\right\\rangle + \\frac{1}{2}\\left\\|x-y\\right\\|^{2}_{A(y)}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2413529c2542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mHH\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmatvecA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mHH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspdiags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsi_second\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_matmat_pass2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import LinearOperator\n",
    "\n",
    "HH =H.T*H\n",
    "def A(x,G,HH,lamb,delta):\n",
    "    matvecA = lambda v : (HH + lamb/2*(G.T*scipy.sparse.spdiags(np.squeeze(psi_second(G*x,delta)),0,M,M))*G)*v\n",
    "    return LinearOperator((N,N), matvec=matvecA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q2.}}$ We propose basic MM algorithm with $\\theta_{n} = 1$ to to minimize $f$\n",
    "$$x_{n+1} = x_{n}  - \\theta_{n}A(x_{n})^{-1}\\nabla f(x_{n})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MM_quadratic(f,grad_f,invA,x0,L,N,y,G,H,lamb,delta,HH):\n",
    "    grad = N*1e-4\n",
    "    x = x0\n",
    "    it = 0\n",
    "    itermax = 10\n",
    "    objectives = []\n",
    "    times = [0]\n",
    "    \n",
    "    while grad > np.sqrt(N)*1e-4 and it <= itermax:         \n",
    "        t1 = time.time()\n",
    "        gradient = grad_f(x,y,G,H,lamb,delta)\n",
    "        x = x - 0.5*scipy.sparse.linalg.bicg(A(x,G,HH,lamb,delta),gradient)\n",
    "        \n",
    "        \n",
    "        grad = np.linalg.norm(gradient)\n",
    "        criterion = f(x,y,G,H,lamb,delta)        \n",
    "        objectives.append(criterion)\n",
    "        \n",
    "        if it%1 == 0:\n",
    "            print(\"Iteration {}, objective: {:.2E}, {}, {}\".format(it,Decimal(criterion),grad,0.5*np.linalg.norm(H*x - y)**2))        \n",
    "        t2 = time.time()\n",
    "        times.append(times[-1]+t2-t1)\n",
    "        it += 1\n",
    "    return x,objectives,times\n",
    "\n",
    "\n",
    "x_hat,objectives,times = MM_quadratic(f,grad_f,A,x0,L,N,y,G,H,lamb,delta,HH)\n",
    "plt.figure()\n",
    "plt.semilogy(times[1:],objectives)\n",
    "plt.grid(which='both')\n",
    "print('\\nSignal to Noise Ratio:',SNR(x_hat,x))\n",
    "plt.figure()\n",
    "plt.imshow(x_hat.reshape(90,90).T,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 3MG  algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMMG(f,grad_f,A,x0,L,N,y,G,H,lamb,delta,HH):\n",
    "    gamma = 1/L\n",
    "    grad = N*1e-4\n",
    "    x = x0\n",
    "    it = 0\n",
    "    itermax = 10\n",
    "    objectives = []\n",
    "    times = [0]\n",
    "    \n",
    "    while grad > np.sqrt(N)*1e-4 and it <= itermax: \n",
    "        t1 = time.time()\n",
    "        gradient = grad_f(x,y,G,H,lamb,delta)\n",
    "        if it == 0:\n",
    "            D = - gradient\n",
    "        else:\n",
    "            D = np.concatenate((-gradient,diff),axis=1)\n",
    "        \n",
    "        temp = np.dot(np.dot(D.T,A(x,G,HH,lamb,delta)),D)\n",
    "        u = - np.dot(np.linalg.pinv(temp),np.dot(D.T,gradient))\n",
    "        x = x + np.dot(D,u)\n",
    "        grad = np.linalg.norm(gradient)\n",
    "        criterion = f(x,y,G,H,lamb,delta)        \n",
    "        objectives.append(criterion)\n",
    "        \n",
    "        if it%1 == 0:\n",
    "            print(\"Iteration {}, objective: {:.2E}, {}\".format(it,Decimal(criterion),grad))        \n",
    "        t2 = time.time()\n",
    "        times.append(times[-1]+t2-t1)\n",
    "        diff = np.dot(D,u)\n",
    "        it += 1\n",
    "    return x,objectives,times\n",
    "        \n",
    "x_hat,objectives,times = MMMG(f,grad_f,A,x0,L,N,y,G,H,lamb,delta,HH)\n",
    "plt.figure()\n",
    "plt.semilogy(times[1:],objectives)\n",
    "plt.grid(which='both')\n",
    "print('\\nSignal to Noise Ratio:',SNR(x_hat,x))\n",
    "plt.figure()\n",
    "plt.imshow(x_hat.reshape(90,90).T,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Block-coordinate MM quadratic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MM_block(f,grad_f,invA,x0,L,N,y,G,H,lamb,delta,J,Nj):\n",
    "    grad = N*1e-4\n",
    "    x = x0\n",
    "    it = 0\n",
    "    itermax = 10\n",
    "    objectives = []\n",
    "    times = [0]\n",
    "    \n",
    "    while grad > np.sqrt(N)*1e-4 and it <= itermax:   \n",
    "        j = it%J + 1\n",
    "        \n",
    "        t1 = time.time()\n",
    "        gradient = grad_f(x,y,G,H,lamb,delta)[Nj*(j-1):J*Nj+1]\n",
    "        \n",
    "        #x = x - 1.99*np.dot(invA,gradient)\n",
    "        #x = x - 1*scipy.sparse.linalg.bicg(A(x,y,G,H,lamb,delta),gradient)\n",
    "        x[Nj*(j-1):J*Nj+1] = x[Nj*(j-1):J*Nj+1] - np.linalg.solve(A(x,G,H,lamb,delta)[Nj*(j-1):J*Nj+1,Nj*(j-1):J*Nj+1],gradient)\n",
    "        #x = x - 1.99*np.linalg.lstsq(A(x,y,G,H,lamb,delta),gradient,rcond=None)\n",
    "        \n",
    "        grad = np.linalg.norm(gradient)\n",
    "        criterion = f(x,y,G,H,lamb,delta)        \n",
    "        objectives.append(criterion)\n",
    "        \n",
    "        if it%1 == 0:\n",
    "            print(\"Iteration {}, objective: {:.2E}, {}, {}\".format(it,Decimal(criterion),grad,0.5*np.linalg.norm(H*x - y)**2))        \n",
    "        t2 = time.time()\n",
    "        times.append(times[-1]+t2-t1)\n",
    "        it += 1\n",
    "    return x,objectives,times\n",
    "        \n",
    "x_hat,objectives,times = MM_block(f,grad_f,A,x0,L,N,y,G,H,lamb,delta,2,N//2)\n",
    "plt.figure()\n",
    "plt.semilogy(times[1:],objectives)\n",
    "plt.grid(which='both')\n",
    "print('\\nSignal to Noise Ratio:',SNR(x_hat,x))\n",
    "plt.figure()\n",
    "plt.imshow(x_hat.reshape(90,90).T,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.  Parallel MM quadratic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MM_parallel(f,grad_f,invA,x0,L,N,y,G,H,lamb,delta):\n",
    "    grad = N*1e-4\n",
    "    x = x0\n",
    "    it = 0\n",
    "    itermax = 10\n",
    "    objectives = []\n",
    "    times = [0]\n",
    "    \n",
    "    while grad > np.sqrt(N)*1e-4 and it <= itermax:   \n",
    "        j = it%J + 1\n",
    "        \n",
    "        t1 = time.time()\n",
    "        gradient = grad_f(x,y,G,H,lamb,delta)[Nj*(j-1):J*Nj+1]\n",
    "        \n",
    "        #x = x - 1.99*np.dot(invA,gradient)\n",
    "        #x = x - 1*scipy.sparse.linalg.bicg(A(x,y,G,H,lamb,delta),gradient)\n",
    "        x[Nj*(j-1):J*Nj+1] = x[Nj*(j-1):J*Nj+1] - 1.99*np.linalg.solve(A(x,G,H,lamb,delta)[Nj*(j-1):J*Nj+1,Nj*(j-1):J*Nj+1],gradient)\n",
    "        #x = x - 1.99*np.linalg.lstsq(A(x,y,G,H,lamb,delta),gradient,rcond=None)\n",
    "        grad = np.linalg.norm(gradient)\n",
    "        criterion = f(x,y,G,H,lamb,delta)        \n",
    "        objectives.append(criterion)\n",
    "        \n",
    "        if it%1 == 0:\n",
    "            print(\"Iteration {}, objective: {:.2E}, {}, {}\".format(it,Decimal(criterion),grad,0.5*np.linalg.norm(H*x - y)**2))        \n",
    "        t2 = time.time()\n",
    "        times.append(times[-1]+t2-t1)\n",
    "        it += 1\n",
    "    return x,objectives,times\n",
    "        \n",
    "x_hat,objectives,times = MM_parallel(f,grad_f,A,x0,L,N,y,G,H,lamb,delta)\n",
    "plt.figure()\n",
    "plt.semilogy(times[1:],objectives)\n",
    "plt.grid(which='both')\n",
    "print('\\nSignal to Noise Ratio:',SNR(x_hat,x))\n",
    "plt.figure()\n",
    "plt.imshow(x_hat.reshape(90,90).T,cmap = 'gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
