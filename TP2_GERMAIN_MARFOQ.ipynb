{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of DOSY NMR signals - Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Generation of synthetic data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q1.}}$ Let's read the signal data from the file \"signal.dat\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "signal = np.zeros(N)\n",
    "\n",
    "filepath = 'signal.txt'  \n",
    "with open(filepath) as fp:  \n",
    "    line = fp.readline()\n",
    "    cnt = 1\n",
    "    while line:\n",
    "        signal[cnt-1] = np.float64(line)\n",
    "        line = fp.readline()\n",
    "        cnt += 1\n",
    "    \n",
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q2.}}$  We create first T using an exponential sampling strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_min = 1 \n",
    "T_max = 1000\n",
    "T = np.array([T_min*np.exp(-n*(np.log(T_min/T_max))/(N-1)) for n in range(N)],dtype=np.float64)\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q3.}}$ Display the original signal $\\bar{x}$ as a function of T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(T),signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q4.}}$  Create t using a regular sampling strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 50 \n",
    "t_min = 0\n",
    "t_max = 1.5\n",
    "t = np.array([t_min + ((m-1)/(M-1))*(t_max - t_min) for m in range(1,M+1)],dtype=np.float64)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\underline{\\textbf{Q4.}}$ Construct matrix K using (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### I should change this (no loop) ###########\n",
    "\n",
    "K = np.zeros((M,N))\n",
    "\n",
    "for n in range(N):\n",
    "    for m in range(M):\n",
    "        K[m,n] = np.exp(- t[m]*T[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank $K = \\min(n,m)$ so $K^\\top K$ is symetric positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q5.}}$ Simulate the noisy data according to model (2), by taking $w \\sim \\mathcal{N} (0, \\sigma^2 I_{M})$ with $\\sigma = 0.01z^{(1)}$ , where $z = K\\bar{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = signal\n",
    "xmin = min(x)\n",
    "xmax = max(x)\n",
    "z = np.dot(K,x)\n",
    "sigma = 0.01*z[0]\n",
    "mean = np.zeros(M)\n",
    "cov  = (sigma**2)*np.eye(M)\n",
    "\n",
    "w = np.random.multivariate_normal(mean, cov)\n",
    "y = z + w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q6.}}$ Display the resulting noisy data y as a function of t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Comparison of regularization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Smoothness prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defintion of D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.eye(N)\n",
    "\n",
    "for n in range(N-1):\n",
    "    D[n+1,n] = -1\n",
    "\n",
    "D[0,N-1] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the problem:\n",
    "\n",
    "$\\inf_{x\\in\\mathbb{R}^N}\\ \\frac{1}{2}\\ \\lVert Kx - y \\rVert^2_2 + \\frac{\\beta}{2}\\ \\lVert Dx \\rVert^2_2 = \\inf_{x\\in\\mathbb{R}^N}\\ J(x)$\n",
    "\n",
    "J is coercive so a minimum exists. J is striclty convex because $\\nabla^2 J = K^\\top K + \\beta\\ D^\\top D$ is symetric positive. Therefore the minimum is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q3.}}$ Solving the optimization problem. The optimality condition gives us the exact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "x_hat = np.dot(np.linalg.inv(np.dot(K.T,K) + beta*np.dot(D.T,D)),np.dot(K.T,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_MSE(x,y):\n",
    "    return( (np.linalg.norm(x-y)/np.linalg.norm(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized_MSE(x_hat,x))\n",
    "plt.plot(np.log(T),x_hat)\n",
    "plt.plot(np.log(T),x)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q4.}}$ Tunning $\\beta$ with a grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_grid = np.exp(np.log(10)*np.linspace(-2,2,5))\n",
    "errors = np.zeros(beta_grid.shape[0])\n",
    "\n",
    "for i in range(beta_grid.shape[0]):\n",
    "    beta = beta_grid[i]\n",
    "    x_hat = np.dot(np.linalg.inv(np.dot(K.T,K) + beta*np.dot(D.T,D)),np.dot(K.T,y))\n",
    "    errors[i] = normalized_MSE(x_hat,x)\n",
    "    print(\"MSE : {}, beta : {}\".format(normalized_MSE(x_hat,x),beta))\n",
    "    plt.plot(np.log(T),x_hat)\n",
    "    plt.plot(np.log(T),x)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothness prior + constraints \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the problem:\n",
    "\n",
    "$\\inf_{x\\in\\mathbb{R}^N}\\ \\frac{1}{2}\\ \\lVert Kx - y \\rVert^2_2 + \\frac{\\beta}{2}\\ \\lVert Dx \\rVert^2_2 + \\iota_{[x_{min},x_{max}]^N}(x)= \\inf_{x\\in[x_{min},x_{max}]^N}\\ J(x)$\n",
    "\n",
    "$[x_{min},x_{max}]^N$ is compact and L is continuous so L admits a minimum on $[x_{min},x_{max}]^N$. L is striclty convex because $\\nabla^2 J = K^\\top K + \\beta\\ D^\\top D$ is symetric positive. Thus, the minimum is unique.\n",
    "\n",
    "We consider a forward-backward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_grid = np.exp(np.log(10)*np.linspace(-3,1,5))\n",
    "\n",
    "def proximal_gradient_smooth_cons(lamb,K,y,T,xmin,xmax,display = False):\n",
    "    tol = 1e-8\n",
    "    m = K.shape[0]\n",
    "    n = K.shape[1]\n",
    "    u = np.zeros((n,1))\n",
    "    w,v = np.linalg.eig(np.dot(K.T,K) + lamb * np.dot(D.T,D))\n",
    "    L = max(w)\n",
    "    epsilon = 1/L\n",
    "    norm2 = np.linalg.norm(np.dot(K,u) - y.reshape(m,1))\n",
    "    normD = np.linalg.norm(np.dot(D,u))\n",
    "    obj = 0.5 * norm2 **2 + 0.5 * lamb * normD ** 2 + ind(u,xmin,xmax)\n",
    "    last_obj = 0\n",
    "    if display:\n",
    "        print(\"Objectif : {}, ||Ku-y||^2 : {}, ||Du||^2 : {}, ind : {}\".format(obj,norm2**2,normD**2,ind(u,xmin,xmax)))\n",
    "    \n",
    "    while abs(obj - last_obj) > tol:\n",
    "        w = u - epsilon * (np.dot(K.T,np.dot(K,u) - y.reshape(m,1)) + lamb * np.dot(D.T,np.dot(D,u)))\n",
    "        last_obj = obj\n",
    "        u = prox_cons(w,xmin,xmax) \n",
    "        norm2 = np.linalg.norm(np.dot(K,u) - y.reshape(m,1))\n",
    "        normD = np.linalg.norm(np.dot(D,u))\n",
    "        obj = 0.5 * norm2 **2 + 0.5*lamb * normD ** 2 + ind(u,xmin,xmax)\n",
    "        if display:\n",
    "            print(\"Objectif : {}, ||Ku-y||^2 : {}, ||Du||^2 : {}, ind : {}\".format(obj,norm2**2,normD**2,ind(u,xmin,xmax)))\n",
    "        \n",
    "    return u\n",
    "        \n",
    "def prox_cons(w,xmin,xmax):\n",
    "    m = w.shape[0]\n",
    "    out = np.zeros((m,1))\n",
    "    for index in range(m):\n",
    "        value = w[index,0]\n",
    "        if value >  xmax:\n",
    "            out[index,0] = xmax\n",
    "        elif value <  xmin:\n",
    "            out[index,0] = xmin\n",
    "        elif value >= xmin and value <= xmax:\n",
    "            out[index,0] = value\n",
    "    return out\n",
    "\n",
    "def ind(x,xmin,xmax):\n",
    "    n = x.shape[0]\n",
    "    for i in range(n):\n",
    "        if x[i] > xmax or x[i] < xmin:\n",
    "            return 1e30\n",
    "    return 0\n",
    "\n",
    "for i in range(beta_grid.shape[0]):\n",
    "    beta = beta_grid[i]\n",
    "    x_hat = proximal_gradient_smooth_cons(beta,K,y,T,xmin,xmax) #,True\n",
    "    print(\"MSE : {}, beta : {}\\n\".format(normalized_MSE(x_hat,x.reshape(200,1)),beta))\n",
    "    plt.plot(np.log(T),x_hat)\n",
    "    plt.plot(np.log(T),x)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the problem:\n",
    "\n",
    "$\\inf_{x\\in\\mathbb{R}^N}\\ \\frac{1}{2}\\ \\lVert Kx - y \\rVert^2_2 + \\beta\\ \\lVert x \\rVert_1 = \\inf_{x\\in\\mathbb{R}^N}\\ G(x)$\n",
    "\n",
    "G is coervive so admits a minimum. G is striclty convex as the sum of two striclty convex functions. Thus, the minimum is unique.\n",
    "\n",
    "We consider a forward-backward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_grid = np.exp(np.log(10)*np.linspace(-4,0,5))\n",
    "\n",
    "def proximal_gradient_norme1(lamb,K,y,T,display = False):\n",
    "    tol = 1e-8\n",
    "    m = K.shape[0]\n",
    "    n = K.shape[1]\n",
    "    w,v = np.linalg.eig(np.dot(K.T,K))\n",
    "    epsilon = 1/np.real(np.max(w))\n",
    "    u = np.zeros((n,1))\n",
    "    norm2 = np.linalg.norm(np.dot(K,u) - y.reshape(m,1))\n",
    "    norm1 = np.linalg.norm(u,1)\n",
    "    obj = 0.5*norm2**2 + lamb * norm1\n",
    "    last_obj = 0\n",
    "    if display:\n",
    "        print(\"Objectif : {}, ||Ku-y||^2 : {}, ||u|| : {}\".format(obj,norm2**2,norm1))\n",
    "    \n",
    "    while abs(obj - last_obj) > tol:\n",
    "        w = u - epsilon * np.dot(K.T,(np.dot(K,u) - y.reshape(m,1)))\n",
    "        last_obj = obj\n",
    "        u = prox_norme1(w,lamb,epsilon) \n",
    "        norm2 = np.linalg.norm(np.dot(K,u) - y.reshape(m,1))\n",
    "        norm1 = np.linalg.norm(u,1)\n",
    "        obj = 0.5*norm2**2 + lamb * norm1\n",
    "        if display:\n",
    "            print(\"Objectif : {}, ||Ku-y||^2 : {}, ||u|| : {}\".format(obj,norm2**2,norm1)) \n",
    "    \n",
    "    return u\n",
    "        \n",
    "def prox_norme1(w,lamb,epsilon):\n",
    "    n = w.shape[0]\n",
    "    out = np.zeros((n,1))\n",
    "    for index in range(n):\n",
    "        value = w[index,0]\n",
    "        if value >  epsilon * lamb :\n",
    "            out[index,0] = value - epsilon * lamb\n",
    "        elif value <  -epsilon * lamb :\n",
    "            out[index,0] = value + epsilon * lamb \n",
    "        elif value >=  -epsilon * lamb and value <= epsilon * lamb:\n",
    "            out[index,0] = 0\n",
    "    return out\n",
    "\n",
    "for i in range(beta_grid.shape[0]):\n",
    "    beta = beta_grid[i]\n",
    "    x_hat = proximal_gradient_norme1(beta,K,y,T)\n",
    "    print(\"MSE : {}, beta : {}\".format(normalized_MSE(x_hat,x.reshape(200,1)),beta))\n",
    "    plt.plot(np.log(T),x_hat)\n",
    "    plt.plot(np.log(T),x)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum entropy problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the problem:\n",
    "\n",
    "$\\inf_{x\\in\\mathbb{R}^N}\\ \\frac{1}{2}\\ \\lVert Kx - y \\rVert^2_2 + \\beta\\ \\mathrm{ent}(x) = \\inf_{x\\in\\mathbb{R}^N}\\ H(x)$\n",
    "\n",
    "The entropy is strictly convex, and proper because its domain is $\\mathbb{R}_+^N\\neq\\emptyset$. It is not differentiable because of the infinite values taken for negative values of the coordinates. The entropy is coercive  and the function norm is positive so $H$ admits a minimum. It is unique by strict convexity.\n",
    "\n",
    "We first use a forward-backward algorithm.\n",
    "\n",
    "$$\\mathrm{prox}_{\\varepsilon\\beta\\ \\mathrm{ent}} (u) = \\left(\\varepsilon\\beta\\ W\\left(\\exp\\left(\\frac{u_i}{\\varepsilon\\beta} - 1 - \\log(\\varepsilon\\beta)\\right)\\right)\\right)_i$$ with the Lambert function W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns W(exp(u)) componentwise\n",
    "def W_exp(u):\n",
    "    \n",
    "    m = u.shape[0]\n",
    "    output = np.zeros((m,1))\n",
    "    \n",
    "    for index in range(m):\n",
    "        val = u[index,0]\n",
    "        \n",
    "        if val >= 100:\n",
    "            output[index,0] = val - np.log(val)\n",
    "    \n",
    "        if val < 100 and val > - 20 :\n",
    "            w = 1\n",
    "            v = 0\n",
    "            while abs(w - v)/abs(w) > 1e-6:\n",
    "                v = w\n",
    "                e = np.exp(w)\n",
    "                f = w*e - np.exp(val)\n",
    "                w = w - f/((e*(w+1) - (w+2)*f/(2*w+2)))\n",
    "            output[index,0] = w\n",
    "            \n",
    "        if val <=-20:\n",
    "            output[index,0] = 0        \n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_grid = np.exp(np.log(10)*np.linspace(-6,0,7))\n",
    "\n",
    "def proximal_gradient_entr(lamb,K,y,T,xmin,xmax,display = False):\n",
    "    tol = 1e-8\n",
    "    m = K.shape[0]\n",
    "    n = K.shape[1]\n",
    "    u = np.zeros((n,1))\n",
    "    w,v = np.linalg.eig(np.dot(K.T,K))\n",
    "    epsilon = 1/np.real(np.max(w))\n",
    "    norm2 = np.linalg.norm(np.dot(K,u) - y.reshape(m,1))\n",
    "    entropy = ent(u)\n",
    "    obj = 0.5 * norm2 **2 + lamb * entropy\n",
    "    last_obj = 0\n",
    "    if display:\n",
    "        print(\"Objectif : {}, ||Ku-y||^2 : {}, entropy : {}\".format(obj,norm2**2,entropy))\n",
    "    \n",
    "    while abs(obj - last_obj) > tol :\n",
    "        w = u - epsilon * np.dot(K.T,np.dot(K,u) - y.reshape(m,1)) \n",
    "        last_obj = obj\n",
    "        u = prox_entr(w/(lamb * epsilon) - (1+np.log(lamb*epsilon))*np.ones(w.shape),lamb,epsilon) \n",
    "        norm2 = np.linalg.norm(np.dot(K,u) - y.reshape(m,1))\n",
    "        entropy = ent(u)\n",
    "        obj = 0.5 * norm2 **2 + lamb * entropy\n",
    "        if display:\n",
    "            print(\"Objectif : {}, ||Ku-y||^2 : {}, entropy : {}\".format(obj,norm2**2,entropy))\n",
    "        \n",
    "    return u\n",
    "        \n",
    "def prox_entr(w,lamb,epsilon):    \n",
    "    return lamb*epsilon*W_exp(w)\n",
    "\n",
    "def ent(x):\n",
    "    tol = 1e-5\n",
    "    n = x.shape[0]\n",
    "    out = np.zeros((x.shape[0],1))\n",
    "    for i in range(n):\n",
    "        value = x[i]\n",
    "        if value > tol:\n",
    "            out[i] = value*np.log(value)\n",
    "        else:\n",
    "            out[i] = 0\n",
    "    return sum(out)[0]\n",
    "        \n",
    "\n",
    "for i in range(beta_grid.shape[0]):\n",
    "    beta = beta_grid[i]\n",
    "    x_hat = proximal_gradient_entr(beta,K,y,T,xmin,xmax) #,True\n",
    "    print(\"MSE : {}, beta : {}\\n\".format(normalized_MSE(x_hat,x.reshape(200,1)),beta))\n",
    "    plt.plot(np.log(T),x_hat)\n",
    "    plt.plot(np.log(T),x)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a Douglas-Rachford algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Douglas Rashford Algorithm**\n",
    "\n",
    "We define $f(x) = \\frac{1}{2}\\left\\|Kx-y\\right\\|^{2} $ and $g(x) = \\beta .\\text{ent}(x) = \\beta.\\sum_{i=1}^{N}\\phi(x^{(n)})$. To implement the Rashford-Douglas algorithm we need to compute $prox_{\\gamma f}$ and $prox_{\\gamma g}$:\n",
    "\n",
    "$$prox_{\\gamma f}(x) = (Id + \\gamma K^{*}K)^{-1}(x+K^{*}y)$$\n",
    "\n",
    "$$prox_{\\gamma g}(x)  = \\left( prox_{\\gamma \\beta \\phi}\\left (x^{(n)}\\right)\\right)_{n=1,..,N}$$\n",
    "\n",
    "Where $prox_{\\gamma \\beta \\phi}\\left( x\\right) =\\beta \\gamma W(e^{\\frac{x}{\\beta \\gamma}-1 -ln(\\beta \\gamma)})$Âµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_gamma_f(x, K, y, gamma = 1):\n",
    "    n = x.shape[0]\n",
    "    return np.linalg.solve(np.eye(n) + gamma*np.dot(K.T,K), x + np.dot(K.T,y).reshape(-1,1))\n",
    "\n",
    "def prox_gamma_ent(x, gamma = 1):\n",
    "    return gamma*W_exp((x/gamma)-1-np.log(gamma))\n",
    "    \n",
    "\n",
    "# We choose lambda = 1/it\n",
    "def douglas_rashford(beta, K, y, x_init, gamma = 1, eps = 1e-3 , Iter_lim = 1000):\n",
    "    it = 0\n",
    "    x = x_init\n",
    "    n = x.shape[0]\n",
    "    matrix_temp = np.linalg.inv(np.eye(n) + gamma*np.dot(K.T,K))\n",
    "    while (it < Iter_lim):\n",
    "        it += 1\n",
    "        y_ = prox_gamma_ent(x, gamma=gamma*beta)\n",
    "        z = prox_gamma_f(2*y_-x, K=K, y=y, gamma=gamma)\n",
    "        x = x + (1/it)*(z-y_)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_grid = np.exp(np.log(10)*np.linspace(-2,2,7))\n",
    "\n",
    "for i in range(beta_grid.shape[0]):\n",
    "    beta = beta_grid[i]\n",
    "    x_init = np.ones((K.shape[1],1))\n",
    "    x_hat = douglas_rashford(beta = beta,K=K,y=y,x_init = x_init) #,True\n",
    "    print(\"MSE : {}, beta : {}\\n\".format(normalized_MSE(x_hat,x.reshape(200,1)),beta))\n",
    "    plt.plot(np.log(T),x_hat)\n",
    "    plt.plot(np.log(T),x)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Q8.}}$  We solve now the optimization problem $\\underset{x \\in \\mathbb{R}^{N}}{\\text{minimize}} \\hspace{0.1cm}\\text{ent(x)}$ subject to $\\left\\|Kx - y\\right\\|^{2} \\leq \\eta M\\sigma^{2}$. This problem is equivalent to $$\\underset{x \\in \\mathbb{R}^{N}}{\\text{minimize}} \\hspace{0.1cm} ent(x) + \\iota_{C}(Kx) \\hspace{1.5cm} (4)$$\n",
    "Where $C = \\left\\{ u \\in \\mathbb{R}^{M} / \\left\\|u -y\\right\\|^{2}\\leq \\eta M \\sigma^{2}\\right\\} = B(y,\\sigma\\sqrt{M\\eta})$ \n",
    "\n",
    "Let's denote $g_{1}(x) = \\text{ent}(x), \\forall x \\in \\mathbb{R}^{N}$ and $g_{2}(u) = \\iota_{C}(u), \\forall u \\in \\mathbb{R}^{M} $. We denote also $L_{1} = Id_{N}$ and $L_{2} = K$. Thus the problem (4) can be written $$\\underset{x \\in \\mathbb{R}^{N}}{\\text{minimize}} \\hspace{0.1cm} g_{1}(L_{1}x) + g_{2}(L_{2}x)$$\n",
    "\n",
    "Thus we can use PPXA to solve this optimization problem, to do so we nedd to compute the proximity operator for $\\gamma g_{1}$ and $\\gamma g_{2}$: \n",
    "\n",
    "$$\\forall x \\in \\mathbb{R}^{N}; prox_{\\gamma g_{1}}(x)  = \\left( prox_{\\gamma  \\phi}\\left (x^{(n)}\\right)\\right)_{n=1,..,N}$$\n",
    "$$\\forall u \\in \\mathbb{R}^{M}; prox_{\\gamma g_{2}}(u) = prox_{\\iota_{C}}(u) = Proj_{B(y,\\sigma\\sqrt{M\\eta})}(u) = y + (u-y).\\text{max}(1,\\frac{r}{\\left\\|u-y\\right\\|}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define of the projection\n",
    "def proj_boule(u, y, eta = 1):\n",
    "    return y + (u-y)*max(1,(sigma*np.sqrt(M*eta))/np.linalg.norm(u-y))\n",
    "    \n",
    "def ppxa(K, y, x_init, gamma = 1, eps = 1e-3 , Iter_lim = 1000):\n",
    "    x_ = x_init\n",
    "    x_1 = x_init\n",
    "    x_2 = np.dot(K,x_1)\n",
    "    v_ = np.linalg.solve(np.dot(K.T,K)  + np.eye(N), x_1 + np.dot(K.T,x_2))\n",
    "    it = 0\n",
    "    while (it < Iter_lim):\n",
    "        it += 1\n",
    "        lambda_ = 1 -1/it\n",
    "        \n",
    "        y_1 = prox_gamma_ent(x_1, gamma=gamma)\n",
    "        y_2 = proj_boule(x_2, y, eta = 1)\n",
    "        \n",
    "        c = np.linalg.solve(np.dot(K.T,K)  + np.eye(N), y_1 + np.dot(K.T,y_2))\n",
    "        \n",
    "        x_1 = x_1 + lambda_*(2*c-v_ - y_1)\n",
    "        x_2 = x_2 + lambda_*(np.dot(K,2*c-v_)-y_2)\n",
    "        \n",
    "        v_= v_ + lambda_*(c-v_)\n",
    "        \n",
    "    return v_\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = np.ones((K.shape[1],1))\n",
    "x_hat = ppxa(K=K,y=y,x_init = x_init) \n",
    "print(\"MSE : {}, beta : {}\\n\".format(normalized_MSE(x_hat,x.reshape(200,1))))\n",
    "plt.plot(np.log(T),x_hat)\n",
    "plt.plot(np.log(T),x)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
